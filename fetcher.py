# -*- coding: utf-8 -*-
"""fetcher.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ozm4RkxfDTXLwY-MZO0eI1PsS1cLfIMt
"""

import pandas as pd
import requests
import io
import zipfile
import os
from datetime import datetime, date, timedelta

# ==========================================
# 1. CONFIGURATION
# ==========================================
# Saving locally inside the repository directory
BASE_PATH = "."

MASTER_CSV_FILE = os.path.join(BASE_PATH, "master_nifty500_cleaned.csv")
MASTER_EXCEL_FILE = os.path.join(BASE_PATH, "master_nifty500_cleaned.xlsx")
CURRENT_TICKER_FILE = os.path.join(BASE_PATH, "ind_nifty500list.csv")

# Headers
NSE_HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36",
    "Accept": "*/*",
    "Referer": "https://www.nseindia.com/all-reports",
    "Connection": "keep-alive"
}

# EXTENDED MAPPING
UDIFF_MAP = {
    'TrdSbl': 'TICKER', 'Symbol': 'TICKER', 'SYMBOL': 'TICKER', 'TckrSymb': 'TICKER',
    'OpnPric': 'OPEN', 'OPEN': 'OPEN', 'Open': 'OPEN',
    'HiPric': 'HIGH', 'HIGH': 'HIGH', 'High': 'HIGH', 'HghPric': 'HIGH',
    'LwPric': 'LOW', 'LOW': 'LOW', 'Low': 'LOW',
    'ClsPric': 'CLOSE', 'CLOSE': 'CLOSE', 'Close': 'CLOSE',
    'TtlTrdVal': 'TURNOVER', 'TOTTRDVAL': 'TURNOVER', 'Total Turnover': 'TURNOVER', 'TtlTrfVal': 'TURNOVER',
    'TtlTradedVol': 'VOLUME', 'VOLUME': 'VOLUME', 'Total Traded Quantity': 'VOLUME', 'TtlTrdQnty': 'VOLUME',
    'SctySrs': 'SERIES', 'SERIES': 'SERIES', 'Series': 'SERIES'
}

# ==========================================
# 2. HELPER FUNCTIONS
# ==========================================
def get_session():
    s = requests.Session()
    s.headers.update(NSE_HEADERS)
    try:
        s.get("https://www.nseindia.com", timeout=10)
    except:
        pass
    return s

def download_day(d, session):
    date_str = d.strftime("%Y%m%d")
    date_old = d.strftime("%d%b%Y").upper()
    year = d.strftime("%Y")
    month = d.strftime("%b").upper()

    urls = [
        f"https://nsearchives.nseindia.com/content/cm/BhavCopy_NSE_CM_0_0_0_{date_str}_F_0000.csv.zip",
        f"https://nsearchives.nseindia.com/content/cm/BhavCopy_NSE_CM_0_0_0_{date_str}_F_0000.csv",
        f"https://nsearchives.nseindia.com/content/historical/EQUITIES/{year}/{month}/cm{date_old}bhav.csv.zip"
    ]

    for url in urls:
        try:
            r = session.get(url, timeout=5)
            if r.status_code == 200:
                content = r.content
                if url.endswith('.zip'):
                    try:
                        with zipfile.ZipFile(io.BytesIO(content)) as z:
                            csv_filename = z.namelist()[0]
                            return pd.read_csv(z.open(csv_filename))
                    except:
                        continue
                else:
                    return pd.read_csv(io.StringIO(content.decode('utf-8')))
        except:
            continue
    return pd.DataFrame()

def find_column_fuzzy(columns, keywords):
    for col in columns:
        for kw in keywords:
            if kw.lower() in col.lower():
                return col
    return None

# ==========================================
# 3. MAIN EXECUTION
# ==========================================
if __name__ == "__main__":
    print(f"--- STARTING DATA APPEND ---")

    # A. LOAD TARGET TICKERS
    if not os.path.exists(CURRENT_TICKER_FILE):
        print(f"Error: {CURRENT_TICKER_FILE} not found. Please ensure it is uploaded to the repo.")
        exit()

    ticker_df = pd.read_csv(CURRENT_TICKER_FILE)
    sym_col = next((c for c in ticker_df.columns if 'Symbol' in c or 'Ticker' in c), None)
    if not sym_col:
        print("Error: Could not find Symbol column in ticker list.")
        exit()
    target_tickers = ticker_df[sym_col].unique().tolist()

    # B. DETERMINE START DATE
    if os.path.exists(MASTER_CSV_FILE):
        master_df = pd.read_csv(MASTER_CSV_FILE)
        try:
            master_df['DATE'] = pd.to_datetime(master_df['DATE'], dayfirst=True)
        except:
            master_df['DATE'] = pd.to_datetime(master_df['DATE'], format='mixed', dayfirst=True)

        last_date = master_df['DATE'].max().date()
        start_date = last_date + timedelta(days=1)
    else:
        print(f"Creating new master file.")
        master_df = pd.DataFrame()
        start_date = date(2020, 1, 1)

    # C. DOWNLOAD LOOP
    end_date = date.today()
    if start_date > end_date:
        print("Data is already up to date!")
    else:
        session = get_session()
        new_rows = []
        current_date = start_date

        while current_date <= end_date:
            if current_date.weekday() < 5:
                print(f"Fetching {current_date}...", end=" ")
                daily_df = download_day(current_date, session)

                if not daily_df.empty:
                    daily_df.columns = daily_df.columns.str.strip()
                    daily_df = daily_df.rename(columns={k:v for k,v in UDIFF_MAP.items() if k in daily_df.columns})

                    for col_name, keywords in [('TICKER', ['Sym', 'Tckr']), ('HIGH', ['High', 'Hgh']),
                                              ('TURNOVER', ['Val', 'Turn', 'Trd']), ('VOLUME', ['Vol', 'Qty', 'Qnt'])]:
                        if col_name not in daily_df.columns:
                            match = find_column_fuzzy(daily_df.columns, keywords)
                            if match: daily_df = daily_df.rename(columns={match: col_name})

                    if 'TICKER' in daily_df.columns:
                        if 'SERIES' in daily_df.columns:
                            daily_df = daily_df[daily_df['SERIES'].isin(['EQ', 'BE'])]

                        daily_df = daily_df[daily_df['TICKER'].isin(target_tickers)]

                        if not daily_df.empty:
                            if 'TURNOVER' not in daily_df.columns: daily_df['TURNOVER'] = 0
                            if 'VOLUME' not in daily_df.columns:
                                daily_df['VOLUME'] = (daily_df['TURNOVER'] / daily_df['CLOSE']).astype(int) if 'CLOSE' in daily_df.columns else 0

                            cols_to_keep = ['TICKER', 'OPEN', 'HIGH', 'LOW', 'CLOSE', 'VOLUME', 'TURNOVER']
                            final_day = daily_df[cols_to_keep].copy()
                            final_day['DATE'] = pd.to_datetime(current_date)
                            new_rows.append(final_day[['DATE'] + cols_to_keep])
                            print(f"Added {len(final_day)} rows")

            current_date += timedelta(days=1)

        # D. SAVE TO REPOSITORY FOLDER
        if new_rows:
            new_df = pd.concat(new_rows, ignore_index=True)
            full_df = pd.concat([master_df, new_df], ignore_index=True)
            full_df = full_df.sort_values(by=['DATE', 'TICKER']).drop_duplicates(subset=['DATE', 'TICKER'], keep='last')

            full_df['DATE'] = full_df['DATE'].dt.strftime('%d-%m-%Y')

            # Export both formats
            full_df.to_csv(MASTER_CSV_FILE, index=False)
            full_df.to_excel(MASTER_EXCEL_FILE, index=False, engine='openpyxl')

            print(f"SUCCESS! Data appended and saved.")
        else:
            print("\nNo new data to save.")

